{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aug.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFCgmZS+Q/x7gjdw+IQPNu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cherevatin/ML/blob/main/Aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fFx0HUQ0Kuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d973f172-03e8-4986-9d5c-93b931523732"
      },
      "source": [
        "import numpy\n",
        "import math\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.constraints import maxnorm\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils, Sequence\n",
        "\n",
        "import albumentations as A\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Устанавливаем seed для повторяемости результатов\n",
        "numpy.random.seed(42)\n",
        "\n",
        "# Размер изображения\n",
        "img_rows, img_cols = 32, 32\n",
        "\n",
        "# Загружаем данные\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Преобразование размерности изображений\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "\n",
        "# Нормализация данных\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# Преобразуем метки в категории\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "AUGM = A.Compose([    \n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "])\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        " \n",
        "    def __init__(self, X_data , y_data, batch_size, for_train, augm, shuffle = True):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data \n",
        "        self.batch_size = batch_size\n",
        "        self.for_train = for_train\n",
        "        self.augm = augm\n",
        "        self.shuffle = shuffle\n",
        "        self.n = 0\n",
        "        self.list_IDs = numpy.arange(len(self.X_data))\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __next__(self):\n",
        "        # Get one batch of data\n",
        "        data = self.__getitem__(self.n)\n",
        "        # Batch index\n",
        "        self.n += 1\n",
        "        # If we have processed the entire dataset then\n",
        "        if self.n >= self.__len__():\n",
        "            self.on_epoch_end()\n",
        "            self.n = 0        \n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches of the dataset\n",
        "        return math.ceil(len(self.indexes)/self.batch_size)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size : (index+1)*self.batch_size]        \n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        \n",
        "        X = self._generate_x(list_IDs_temp)\n",
        "        \n",
        "        if self.for_train:\n",
        "            y = self._generate_y(list_IDs_temp)\n",
        "            return X, y\n",
        "        else:\n",
        "            return X\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = numpy.arange(len(self.X_data))\n",
        "        if self.shuffle: \n",
        "            numpy.random.shuffle(self.indexes)    \n",
        "            \n",
        "    def _generate_x(self, list_IDs_temp):\n",
        "        X = numpy.zeros((self.batch_size, 32, 32, 3), dtype=numpy.float32)\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            X[i] = self.X_data[ID]\n",
        "            if self.augm:                \n",
        "                img_augm = AUGM(image=X[i])\n",
        "                X[i] = img_augm[\"image\"]                \n",
        "        return X\n",
        "    \n",
        "    def _generate_y(self, list_IDs_temp):\n",
        "        #y = np.empty(self.batch_size)        \n",
        "        y = numpy.zeros((self.batch_size, 10), dtype=numpy.float32)\n",
        "        for i, ID in enumerate(list_IDs_temp):            \n",
        "            y[i] = self.y_data[ID]            \n",
        "        return y\n",
        "\n",
        "\n",
        "MODELS_COUNT = 5;\n",
        "y_pred = None\n",
        "\n",
        "for i in range (MODELS_COUNT):\n",
        "  # Создаем последовательную модель\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                  activation='relu',\n",
        "                  input_shape=(32,32,3),padding = \"same\"))\n",
        "  model.add(Conv2D(64, kernel_size=(4, 4),\n",
        "                  activation='relu',\n",
        "                   padding = \"same\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu',padding = \"same\"))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "  model.add(Dropout(0.2)) \n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(500, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "  # Компилируем модель\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  # print(model.summary())\n",
        "  \n",
        "  # Обучаем сеть\n",
        "  # model.fit(X_train, Y_train, batch_size=200, epochs=25, validation_split=0.1, verbose=2)\n",
        "  train_generator = DataGenerator(X_train, Y_train, batch_size = 200, \n",
        "                                    for_train=True, augm = True, shuffle=True)\n",
        "  \n",
        "  val_generator = DataGenerator(X_val, Y_val, batch_size = 200, \n",
        "                                    for_train=True, augm = False, shuffle=False)\n",
        "  \n",
        "  history = model.fit(train_generator,validation_data=val_generator,epochs=30, verbose=1)\n",
        "  # Оцениваем качество обучения сети на тестовых данных\n",
        "  scores = model.evaluate(X_test, Y_test, verbose=0)\n",
        "  print(\"Точность работы на тестовых данных: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "  if y_pred is None:\n",
        "    y_pred = model.predict(X_test)\n",
        "  else:\n",
        "    y_pred += model.predict(X_test)\n",
        "\n",
        "y_pred /= MODELS_COUNT\n",
        "pred = numpy.argmax(y_pred, axis=1)\n",
        "acc_test = accuracy_score(y_test, pred)\n",
        "\n",
        "print(\"Final accuracy: {}%\".format(round(acc_test*100,2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "200/200 [==============================] - 472s 2s/step - loss: 2.0181 - accuracy: 0.2513 - val_loss: 1.4112 - val_accuracy: 0.4893\n",
            "Epoch 2/30\n",
            "200/200 [==============================] - 469s 2s/step - loss: 1.5078 - accuracy: 0.4537 - val_loss: 1.1775 - val_accuracy: 0.5825\n",
            "Epoch 3/30\n",
            "200/200 [==============================] - 468s 2s/step - loss: 1.3433 - accuracy: 0.5199 - val_loss: 1.0648 - val_accuracy: 0.6253\n",
            "Epoch 4/30\n",
            "200/200 [==============================] - 469s 2s/step - loss: 1.2151 - accuracy: 0.5677 - val_loss: 0.9394 - val_accuracy: 0.6646\n",
            "Epoch 5/30\n",
            "200/200 [==============================] - 471s 2s/step - loss: 1.1295 - accuracy: 0.5981 - val_loss: 0.9017 - val_accuracy: 0.6800\n",
            "Epoch 6/30\n",
            "200/200 [==============================] - 472s 2s/step - loss: 1.0702 - accuracy: 0.6198 - val_loss: 0.8268 - val_accuracy: 0.7095\n",
            "Epoch 7/30\n",
            "200/200 [==============================] - 471s 2s/step - loss: 1.0253 - accuracy: 0.6400 - val_loss: 0.7907 - val_accuracy: 0.7222\n",
            "Epoch 8/30\n",
            "200/200 [==============================] - 472s 2s/step - loss: 0.9743 - accuracy: 0.6595 - val_loss: 0.7708 - val_accuracy: 0.7282\n",
            "Epoch 9/30\n",
            "200/200 [==============================] - 474s 2s/step - loss: 0.9570 - accuracy: 0.6651 - val_loss: 0.7347 - val_accuracy: 0.7381\n",
            "Epoch 10/30\n",
            "200/200 [==============================] - 471s 2s/step - loss: 0.9206 - accuracy: 0.6761 - val_loss: 0.7025 - val_accuracy: 0.7516\n",
            "Epoch 11/30\n",
            "200/200 [==============================] - 467s 2s/step - loss: 0.8960 - accuracy: 0.6887 - val_loss: 0.6845 - val_accuracy: 0.7585\n",
            "Epoch 12/30\n",
            "200/200 [==============================] - 469s 2s/step - loss: 0.8735 - accuracy: 0.6943 - val_loss: 0.6718 - val_accuracy: 0.7711\n",
            "Epoch 13/30\n",
            "200/200 [==============================] - 466s 2s/step - loss: 0.8557 - accuracy: 0.6992 - val_loss: 0.6905 - val_accuracy: 0.7650\n",
            "Epoch 14/30\n",
            "200/200 [==============================] - 466s 2s/step - loss: 0.8290 - accuracy: 0.7117 - val_loss: 0.6331 - val_accuracy: 0.7825\n",
            "Epoch 15/30\n",
            "200/200 [==============================] - 465s 2s/step - loss: 0.8218 - accuracy: 0.7112 - val_loss: 0.6227 - val_accuracy: 0.7855\n",
            "Epoch 16/30\n",
            "200/200 [==============================] - 466s 2s/step - loss: 0.7948 - accuracy: 0.7265 - val_loss: 0.5995 - val_accuracy: 0.7902\n",
            "Epoch 17/30\n",
            "200/200 [==============================] - 465s 2s/step - loss: 0.7836 - accuracy: 0.7304 - val_loss: 0.6131 - val_accuracy: 0.7907\n",
            "Epoch 18/30\n",
            "200/200 [==============================] - 465s 2s/step - loss: 0.7764 - accuracy: 0.7321 - val_loss: 0.5717 - val_accuracy: 0.8013\n",
            "Epoch 19/30\n",
            "200/200 [==============================] - 467s 2s/step - loss: 0.7489 - accuracy: 0.7415 - val_loss: 0.5928 - val_accuracy: 0.7948\n",
            "Epoch 20/30\n",
            "200/200 [==============================] - 469s 2s/step - loss: 0.7551 - accuracy: 0.7365 - val_loss: 0.6072 - val_accuracy: 0.7919\n",
            "Epoch 21/30\n",
            "200/200 [==============================] - 469s 2s/step - loss: 0.7330 - accuracy: 0.7422 - val_loss: 0.5725 - val_accuracy: 0.8012\n",
            "Epoch 22/30\n",
            "200/200 [==============================] - 470s 2s/step - loss: 0.7371 - accuracy: 0.7457 - val_loss: 0.5922 - val_accuracy: 0.7983\n",
            "Epoch 23/30\n",
            "200/200 [==============================] - 474s 2s/step - loss: 0.7280 - accuracy: 0.7491 - val_loss: 0.5805 - val_accuracy: 0.7996\n",
            "Epoch 24/30\n",
            "200/200 [==============================] - 471s 2s/step - loss: 0.7027 - accuracy: 0.7587 - val_loss: 0.5566 - val_accuracy: 0.8113\n",
            "Epoch 25/30\n",
            "200/200 [==============================] - 473s 2s/step - loss: 0.7037 - accuracy: 0.7593 - val_loss: 0.5526 - val_accuracy: 0.8125\n",
            "Epoch 26/30\n",
            "200/200 [==============================] - 471s 2s/step - loss: 0.6937 - accuracy: 0.7565 - val_loss: 0.5351 - val_accuracy: 0.8178\n",
            "Epoch 27/30\n",
            "200/200 [==============================] - 472s 2s/step - loss: 0.6847 - accuracy: 0.7653 - val_loss: 0.5245 - val_accuracy: 0.8237\n",
            "Epoch 28/30\n",
            "200/200 [==============================] - 471s 2s/step - loss: 0.6807 - accuracy: 0.7639 - val_loss: 0.5339 - val_accuracy: 0.8170\n",
            "Epoch 29/30\n",
            "200/200 [==============================] - 473s 2s/step - loss: 0.6688 - accuracy: 0.7687 - val_loss: 0.5306 - val_accuracy: 0.8190\n",
            "Epoch 30/30\n",
            "200/200 [==============================] - 473s 2s/step - loss: 0.6693 - accuracy: 0.7673 - val_loss: 0.5657 - val_accuracy: 0.8167\n",
            "Точность работы на тестовых данных: 80.36%\n",
            "Epoch 1/30\n",
            " 50/200 [======>.......................] - ETA: 5:45 - loss: 2.2233 - accuracy: 0.1720"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}